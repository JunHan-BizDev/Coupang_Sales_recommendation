{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c9545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "from pyspark.sql.session import SparkSession\n",
    "# spark세션 인스턴스\n",
    "spark = SparkSession.builder.appName('master').getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import desc, asc\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d569b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywebhdfs.webhdfs import PyWebHdfsClient\n",
    "from pprint import pprint\n",
    " \n",
    "\n",
    "# path안에 있는 파일 및 폴더이름을 리스트로 반환한다\n",
    "# 단 path는 'user/hadoop/data_analysis/coupang_data/' 와 같이 http를 기입하지 않는다\n",
    "def get_dir(path):\n",
    "    hdfs = PyWebHdfsClient(host='localhost',port='50070', user_name='master')  # your Namenode IP & username here\n",
    "    my_dir = path\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(hdfs.list_dir(my_dir)['FileStatuses']['FileStatus'])):\n",
    "        result.append(hdfs.list_dir(my_dir)['FileStatuses']['FileStatus'][i]['pathSuffix'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# pandas dataframe으로 csv파일을 열고 dataframe을 반환한다\n",
    "# 단 path는 'user/' 로 시작\n",
    "def open_pd_csv(path):\n",
    "    file_path = 'hdfs://192.168.0.9:9000/' + path\n",
    "    data = spark.read.csv(file_path, header=True)\n",
    "    result = data.toPandas()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# pandas dataframe을 path에 저장한다\n",
    "# 단 path는 'user/' 로 시작 , 파일명까지 포함 ex) uesr/hadoop/test.csv\n",
    "def save_pd_csv(df ,path):\n",
    "    file_path = 'hdfs://192.168.0.9:9000/' + path\n",
    "    data = spark.createDataFrame(df)\n",
    "    data.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(file_path)\n",
    "    \n",
    "# spark datafrmae schema 설정\n",
    "def ad_schema(df):\n",
    "    intCols = ['rank','category','price', 'discount_percentage', 'rating_total_count',\n",
    "               'reviews_for_last1year', 'sales']\n",
    "    doubleCols = ['rating']\n",
    "    boolCols = ['rocket_delivery', 'is_out_of_stock']\n",
    "        \n",
    "    for c in df.columns:\n",
    "        type_col = StringType()\n",
    "        if c in intCols:\n",
    "            type_col = IntegerType()\n",
    "        elif c in doubleCols:\n",
    "            type_col = DoubleType()\n",
    "        elif c in boolCols:\n",
    "            type_col = BooleanType()\n",
    "        \n",
    "        df = df.withColumn(c,df[c].cast(type_col))\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a3721",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 판매량 산정 후 결과 저장\n",
    "basic_folder = 'user/hadoop/data_analysis/merged_data/'\n",
    "file_list = get_dir(basic_folder)\n",
    "\n",
    "for file in file_list[:1]:\n",
    "    file_name = basic_folder + file\n",
    "\n",
    "    # 스파크 dataframe 생성(csv파일 읽기)\n",
    "    df = spark.read.csv('hdfs://192.168.0.9:9000/' + file_name, header=True)\n",
    "    df.show()\n",
    "    \n",
    "    # 스키마 정의\n",
    "    intCols = ['rank','category','price', 'discount_percentage', 'rating_total_count',\n",
    "                  'reviews_for_last1year']\n",
    "    doubleCols = ['rating']\n",
    "    boolCols = ['rocket_delivery', 'is_out_of_stock']\n",
    "        \n",
    "    for c in df.columns:\n",
    "        type_col = StringType()\n",
    "        if c in intCols:\n",
    "            type_col = IntegerType()\n",
    "        elif c in doubleCols:\n",
    "            type_col = DoubleType()\n",
    "        elif c in boolCols:\n",
    "            type_col = BooleanType()\n",
    "        \n",
    "        df = df.withColumn(c,df[c].cast(type_col))\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # 카테고리 출력\n",
    "    category = file[:file.find('_')]\n",
    "    print('카테고리 : ',category)\n",
    "\n",
    "    # 판매량 산정\n",
    "    temp = df.select('reviews_for_last1year').sort(asc('reviews_for_last1year'))\n",
    "    temp = df.select('reviews_for_last1year').filter(df.reviews_for_last1year > 0).\\\n",
    "    sort(asc('reviews_for_last1year')).collect()[0:10]\n",
    "    \n",
    "    std = 0\n",
    "    for r in temp :\n",
    "        std += r['reviews_for_last1year']\n",
    "    std = std/10\n",
    "\n",
    "    df = df.withColumn('sales', (1080 - df.rank)*std + (df.reviews_for_last1year)*3)\n",
    "    \n",
    "    # 결과 저장(parquet로 저장)\n",
    "    file_path = 'hdfs://192.168.0.9:9000/' + 'user/hadoop/data_analysis/result2/' + category + '.parquet'\n",
    "    df.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").format(\"parquet\").save(file_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2377907",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 카테고리명과 product_id리스트를 받아서 공통 요소 출력하기\n",
    "\n",
    "category = 'bed'\n",
    "id_list = ['295552610', '60659732', '218912675', '327985427', '1370896429', \n",
    "           '4830312158', '2358790214', '46172654', '110925026', '1105311096', \n",
    "           '1401559740', '1096195704', '2627325', '1129835', '2083440667', \n",
    "           '2240204468', '1472027989', '1801552884', '1829493113', '1555433138', \n",
    "           '32023744', '5471866788', '1914536362', '51417867', '57063884', \n",
    "           '2627331', '1829493909', '1670223623', '145323302', '5335598611', \n",
    "           '1911251180', '4354081837', '5410866415', '1119670850', '78757060', \n",
    "           '4513411171', '1579676541', '5125772618', '1713348', '1539425008', \n",
    "           '5320324198', '5526939920', '4604849960', '286156541', '5359524617', \n",
    "           '133484986', '1790298133', '5417255169', '2035043294', '5125849391', \n",
    "           '1816492647', '5347898691', '169613853', '1141993983', '5369381450', \n",
    "           '133097891', '1358436262', '1458760', '313670425', '1336182884', \n",
    "           '1194148073', '1315500107', '90440244', '5301820049', '2087915123', \n",
    "           '51860502', '60951423', '5009754884', '1984949332', '30267111', '12372816', \n",
    "           '29865253', '4362832164', '115130948', '1758838803', '5339561980', \n",
    "           '2287140789', '285454538', '1960868118', '2035043478', '341306883', \n",
    "           '1357872956', '5072181289', '49300701', '41068685', '2354751719', \n",
    "           '174679218', '2082645126', '2196909053', '129157588', '4356581421', \n",
    "           '1713311', '2044209949', '136081082', '3226430', '4526702040', \n",
    "           '97627864', '120008773', '255068544']\n",
    "\n",
    "\n",
    "# 해당하는 카테고리의 데이터프레임 가져오기(parquet파일 읽기)\n",
    "file_name = 'user/hadoop/data_analysis/result2/' + category + '.parquet'\n",
    "df = spark.read.parquet('hdfs://192.168.0.9:9000/' + file_name)\n",
    "df = ad_schema(df)\n",
    "\n",
    "# id_list에 있는 상품들로 dataframe만들기\n",
    "df_target = spark.createDataFrame(id_list, StringType())\n",
    "df_target = df_target.selectExpr(\"value as product_id\")\n",
    "df_target = df.join(df_target, 'product_id', how='inner')\n",
    "\n",
    "\n",
    "# 공통 요소\n",
    "commonFactors = []\n",
    "\n",
    "# 최다공통 요소 출력\n",
    "for col in df_target.columns[13:len(df_target.columns)-1]:\n",
    "#     print('특성 :', col)\n",
    "    result = df_target.groupby(col).agg(f.count(col).alias(\"개수\"),\n",
    "                                 f.sum('sales').alias('판매량')).sort(desc('개수'))\n",
    "    result = result.filter((f.col(col) != 'NULL'))\n",
    "\n",
    "    if result.count() != 0:\n",
    "        key = str(result.select(col).take(1)[0])\n",
    "        key = key[key.find(\"=\")+2:key.find(\")\")-1]\n",
    "#         count = str(result.select(\"개수\").take(1)[0])\n",
    "        count = count[count.find(\"(\")+1:key.find(\")\")]\n",
    "        commonFactors.append(col+ ' : ' +key)\n",
    "\n",
    "#     result.show()\n",
    "print(\"공통 요소 :\", commonFactors)\n",
    "print('-------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185e52e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
